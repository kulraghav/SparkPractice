{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "sc = SparkContext('local[*]', 'pyspark tutorial')\n",
    "from time import sleep, time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A decorator: timer \n",
    "https://codereview.stackexchange.com/questions/169870/decorator-to-measure-execution-time-of-a-function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from time import time\n",
    "def timer(f):\n",
    "    def wrapper(*args, **kwargs):  \n",
    "        # 1. what are *args and **kwargs  ?  \n",
    "        start = time()\n",
    "        result = f(*args, **kwargs)\n",
    "        end = time()\n",
    "        print 'The function {0} took {1} seconds to return.'.format(f.__name__, end-start)\n",
    "        # 2. what is .format ?\n",
    "        # 3. what is f.__name__ ?\n",
    "        return result\n",
    "    return wrapper\n",
    "# issue: print command is executed on every recursive call of f (ideally it should be executed only once at the end)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. https://www.programiz.com/python-programming/args-and-kwargs\n",
    "   - args (non keyword arguments) is a list \n",
    "   - kwargs (keyword arguments) is a dictionary \n",
    "2. https://pyformat.info/\n",
    "3. https://stackoverflow.com/questions/251464/how-to-get-a-function-name-as-a-string-in-python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating a function on multiple inputs in parallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "_limit = 8\n",
    "my_list = range(_limit)\n",
    "\n",
    "def fun(x):\n",
    "    sleep(5)\n",
    "    return x*x\n",
    "\n",
    "@timer\n",
    "def evaluate_parallel(my_list):\n",
    "    my_rdd = sc.parallelize(my_list)\n",
    "    my_rdd_transformed = my_rdd.map(lambda x: fun(x)).collect()\n",
    "    return list(my_rdd_transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the function evaluate_parallel took 10.0906891823 time\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0, 1, 4, 9, 16, 25, 36, 49]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_parallel(my_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apply a filter on a parallel collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from random import random\n",
    "_num_samples = 1000\n",
    "\n",
    "def is_inside(p):\n",
    "    sleep(0.01)\n",
    "    x, y = random(), random()\n",
    "    return x*x + y*y < 1\n",
    "\n",
    "@timer\n",
    "def get_pi():\n",
    "    count = sc.parallelize(xrange(0, _num_samples)).filter(is_inside).count() # 1. what is xrange ?\n",
    "    print \"Pi is roughly %f\" % (4.0 * count / _num_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. range vs xrange\n",
    "    - https://www.geeksforgeeks.org/range-vs-xrange-python/\n",
    "    - https://stackoverflow.com/questions/94935/what-is-the-difference-between-range-and-xrange-functions-in-python-2-x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pi is roughly 3.120000\n",
      "The function get_pi took 2.86020493507 seconds to return.\n"
     ]
    }
   ],
   "source": [
    "get_pi()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The pagerank algorithm \n",
    "- https://apache.googlesource.com/spark/+/master/examples/src/main/python/pagerank.py\n",
    "- https://developers.soundcloud.com/blog/pagerank-in-spark\n",
    "- https://spark.apache.org/docs/latest/quick-start.html\n",
    "- https://jaceklaskowski.gitbooks.io/mastering-apache-spark/content/spark-rdd-caching.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "my_rdd = sc.parallelize(my_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = sc.textFile('graph.txt')\n",
    "# 1. what is the difference between sc.parallelize() and sc.textFile() ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. sc.textFile creates a parallel collection similar to parallelize()\n",
    "   - https://stackoverflow.com/questions/44860973/what-are-the-differences-between-sc-parallelize-and-sc-textfile\n",
    "   - https://www.quora.com/Is-my-understanding-to-parallel-operations-in-spark-correct\n",
    "   - https://www.learningjournal.guru/article/apache-spark/apache-spark-parallel-processing/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'1', <pyspark.resultiterable.ResultIterable at 0x1163356d0>),\n",
       " (u'3', <pyspark.resultiterable.ResultIterable at 0x116337f50>),\n",
       " (u'2', <pyspark.resultiterable.ResultIterable at 0x116362050>),\n",
       " (u'4', <pyspark.resultiterable.ResultIterable at 0x1163620d0>)]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_pair(line):\n",
    "    line = line.strip()\n",
    "    return line.split()[0], line.split()[1]\n",
    "\n",
    "links = lines.map(lambda line: get_pair(line)).distinct().groupByKey().cache()\n",
    "links.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'1', 1.0), (u'3', 1.0), (u'2', 1.0), (u'4', 1.0)]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ranks = links.map(lambda url_neighbors: (url_neighbors[0], 1.0))\n",
    "ranks.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'2',\n",
       "  (<pyspark.resultiterable.ResultIterable at 0x116352dd0>,\n",
       "   0.23153811726092793)),\n",
       " (u'4',\n",
       "  (<pyspark.resultiterable.ResultIterable at 0x116352fd0>,\n",
       "   0.3299418170968223)),\n",
       " (u'1',\n",
       "  (<pyspark.resultiterable.ResultIterable at 0x116335c10>,\n",
       "   0.38370878711024914)),\n",
       " (u'3',\n",
       "  (<pyspark.resultiterable.ResultIterable at 0x116335710>,\n",
       "   0.3299418170968223))]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "contribs = links.join(ranks)\n",
    "contribs.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 has rank: 0.38370878711.\n",
      "10 has rank: 0.371763389527.\n",
      "3 has rank: 0.329941817097.\n",
      "4 has rank: 0.329941817097.\n",
      "6 has rank: 0.243483514844.\n",
      "5 has rank: 0.243483514844.\n",
      "2 has rank: 0.231538117261.\n"
     ]
    }
   ],
   "source": [
    "_num_iterations = 10\n",
    "\n",
    "from operator import add\n",
    "\n",
    "def computeContribs(urls, rank):\n",
    "    \"\"\"Calculates URL contributions to the rank of other URLs.\"\"\"\n",
    "    num_urls = len(urls)\n",
    "    for url in urls:\n",
    "        yield (url, rank / num_urls)\n",
    "\n",
    "for iteration in range(_num_iterations):\n",
    "        # Calculates URL contributions to the rank of other URLs.  \n",
    "        contribs = links.join(ranks).flatMap(\n",
    "            lambda url_urls_rank: computeContribs(url_urls_rank[1][0], url_urls_rank[1][1]))\n",
    "        # 1. what is flatMap ?\n",
    "        \n",
    "        # Re-calculates URL ranks based on neighbor contributions.\n",
    "        ranks = contribs.reduceByKey(add).mapValues(lambda rank: rank * 0.85 + 0.15)\n",
    "\n",
    "        \n",
    "# Collects all URL ranks and dump them to console.\n",
    "for (link, rank) in ranks.sortBy(lambda x: -x[1]).collect():\n",
    "        print(\"%s has rank: %s.\" % (link, rank))\n",
    "    \n",
    "# 2. How to sort by ranks ?    \n",
    "# 3. How to sort by values in descending order ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. http://www.brunton-spall.co.uk/post/2011/12/02/map-map-and-flatmap-in-scala/\n",
    "2. https://stackoverflow.com/questions/33706408/how-to-sort-by-value-efficiently-in-pyspark\n",
    "3. https://stackoverflow.com/questions/30787635/takeordered-descending-pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
